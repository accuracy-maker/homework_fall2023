{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding of Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient & Imitation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Gradient formula is similar with Imiation Learning. In the Imitation Learning, we aim to maximum the likelihood between the parametered policy and the expert policy and we plan to minium the objective function of reinforcement learning in Policy Gradient.\n",
    "\n",
    "Policy Gradient\n",
    "\n",
    "$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N (\\sum_{t=1}^T \\nabla_\\theta log \\pi_\\theta(a_{i,t}|s_{i,t}))(\\sum_{t=1}^T r(s_{i,t}|a_{i,t}))$\n",
    "\n",
    "Maximum Likelihood\n",
    "\n",
    "$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N (\\sum_{t=1}^T \\nabla_\\theta log \\pi_\\theta(a_{i,t}|s_{i,t}))$\n",
    "\n",
    "It seems that Policy Gradient is weighted Maximun Likelihood. In policy gradient. There are not expert data to teach the policy so it need the cummulative return to tell it whether the action is good or bad.\n",
    "\n",
    "Therefore, they are different considering sampling action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPolicy(nn.Module):\n",
    "    \"\"\"Base MLP policy, which can take an observation and output a distribution over actions.\n",
    "\n",
    "    This class should implement the `forward` and `get_action` methods. The `update` method should be written in the\n",
    "    subclasses, since the policy update rule differs for different algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ac_dim: int,\n",
    "        ob_dim: int,\n",
    "        discrete: bool,\n",
    "        n_layers: int,\n",
    "        layer_size: int,\n",
    "        learning_rate: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if discrete:\n",
    "            self.logits_net = ptu.build_mlp(\n",
    "                input_size=ob_dim,\n",
    "                output_size=ac_dim,\n",
    "                n_layers=n_layers,\n",
    "                size=layer_size,\n",
    "            ).to(ptu.device)\n",
    "            parameters = self.logits_net.parameters()\n",
    "        else:\n",
    "            self.mean_net = ptu.build_mlp(\n",
    "                input_size=ob_dim,\n",
    "                output_size=ac_dim,\n",
    "                n_layers=n_layers,\n",
    "                size=layer_size,\n",
    "            ).to(ptu.device)\n",
    "            self.logstd = nn.Parameter(\n",
    "                torch.zeros(ac_dim, dtype=torch.float32, device=ptu.device)\n",
    "            )\n",
    "            parameters = itertools.chain([self.logstd], self.mean_net.parameters())\n",
    "\n",
    "        self.optimizer = optim.Adam(\n",
    "            parameters,\n",
    "            learning_rate,\n",
    "        )\n",
    "\n",
    "        self.discrete = discrete\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, obs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Takes a single observation (as a numpy array) and returns a single action (as a numpy array).\"\"\"\n",
    "        # TODO: implement get_action\n",
    "        obs = ptu.from_numpy(obs)\n",
    "        action_distribution = self.forward(obs)\n",
    "        if self.discrete:\n",
    "            action = action_distribution.sample()\n",
    "        else:\n",
    "            action = action_distribution.rsample()   \n",
    "\n",
    "        return ptu.to_numpy(action)\n",
    "\n",
    "    def forward(self, obs: torch.FloatTensor):\n",
    "        \"\"\"\n",
    "        This function defines the forward pass of the network.  You can return anything you want, but you should be\n",
    "        able to differentiate through it. For example, you can return a torch.FloatTensor. You can also return more\n",
    "        flexible objects, such as a `torch.distributions.Distribution` object. It's up to you!\n",
    "        \"\"\"\n",
    "        if self.discrete:\n",
    "            # TODO: define the forward pass for a policy with a discrete action space.\n",
    "            logits = self.logits_net(obs)\n",
    "            dist = distributions.Categorical(F.softmax(logits))\n",
    "            \n",
    "        else:\n",
    "            # TODO: define the forward pass for a policy with a continuous action space.\n",
    "            mean = self.mean_net(obs)\n",
    "            std = torch.exp(self.logstd)\n",
    "            dist = distributions.Normal(mean,std)\n",
    "        return dist\n",
    "\n",
    "    def update(self, obs: np.ndarray, actions: np.ndarray, *args, **kwargs) -> dict:\n",
    "        \"\"\"Performs one iteration of gradient descent on the provided batch of data.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class MLPPolicyPG(MLPPolicy):\n",
    "    \"\"\"Policy subclass for the policy gradient algorithm.\"\"\"\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        actions: np.ndarray,\n",
    "        advantages: np.ndarray,\n",
    "    ) -> dict:\n",
    "        \"\"\"Implements the policy gradient actor update.\"\"\"\n",
    "        obs = ptu.from_numpy(obs)\n",
    "        actions = ptu.from_numpy(actions)\n",
    "        advantages = ptu.from_numpy(advantages)\n",
    "\n",
    "        # TODO: implement the policy gradient actor update.\n",
    "        self.optimizer.zero_grad()\n",
    "        action_distribution = self.forward(obs)\n",
    "        if self.discrete:\n",
    "            log_prob = action_distribution.log_prob(actions)\n",
    "        else:\n",
    "            log_prob = action_distribution.log_prob(actions).sum(dim=-1)\n",
    "            \n",
    "        loss = - (log_prob * advantages).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return {\n",
    "            \"Actor Loss\": ptu.to_numpy(loss),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imitation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPolicySL(BasePolicy, nn.Module, metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    Defines an MLP for supervised learning which maps observations to continuous\n",
    "    actions.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    mean_net: nn.Sequential\n",
    "        A neural network that outputs the mean for continuous actions\n",
    "    logstd: nn.Parameter\n",
    "        A separate parameter to learn the standard deviation of actions\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward:\n",
    "        Runs a differentiable forwards pass through the network\n",
    "    update:\n",
    "        Trains the policy with a supervised learning objective\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 ac_dim,\n",
    "                 ob_dim,\n",
    "                 n_layers,\n",
    "                 size,\n",
    "                 learning_rate=1e-4,\n",
    "                 training=True,\n",
    "                 nn_baseline=False,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # init vars\n",
    "        self.ac_dim = ac_dim\n",
    "        self.ob_dim = ob_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.size = size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.training = training\n",
    "        self.nn_baseline = nn_baseline\n",
    "\n",
    "        self.mean_net = build_mlp(\n",
    "            input_size=self.ob_dim,\n",
    "            output_size=self.ac_dim,\n",
    "            n_layers=self.n_layers, size=self.size,\n",
    "        )\n",
    "        self.mean_net.to(ptu.device)\n",
    "        self.logstd = nn.Parameter(\n",
    "\n",
    "            torch.zeros(self.ac_dim, dtype=torch.float32, device=ptu.device)\n",
    "        )\n",
    "        self.logstd.to(ptu.device)\n",
    "        self.optimizer = optim.Adam(\n",
    "            itertools.chain([self.logstd], self.mean_net.parameters()),\n",
    "            self.learning_rate\n",
    "        )\n",
    "\n",
    "    def save(self, filepath):\n",
    "        \"\"\"\n",
    "        :param filepath: path to save MLP\n",
    "        \"\"\"\n",
    "        torch.save(self.state_dict(), filepath)\n",
    "\n",
    "    def forward(self, observation: torch.FloatTensor) -> Any:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network\n",
    "\n",
    "        :param observation: observation(s) to query the policy\n",
    "        :return:\n",
    "            action: sampled action(s) from the policy\n",
    "        \"\"\"\n",
    "        # TODO: implement the forward pass of the network.\n",
    "        # You can return anything you want, but you should be able to differentiate\n",
    "        # through it. For example, you can return a torch.FloatTensor. You can also\n",
    "        # return more flexible objects, such as a\n",
    "        # `torch.distributions.Distribution` object. It's up to you!\n",
    "        observation = observation.float().to(ptu.device)\n",
    "        mean = self.mean_net(observation)\n",
    "        std = torch.exp(self.logstd)\n",
    "        dist = distributions.Normal(mean,std)\n",
    "        action = dist.rsample() # test rsample() & sample()\n",
    "        # test `get action operation` whether need under `torch.no_grad()`\n",
    "        # convert tensor to numpy\n",
    "        return action\n",
    "        \n",
    "\n",
    "    def update(self, observations, actions):\n",
    "        \"\"\"\n",
    "        Updates/trains the policy\n",
    "\n",
    "        :param observations: observation(s) to query the policy\n",
    "        :param actions: actions we want the policy to imitate\n",
    "        :return:\n",
    "            dict: 'Training Loss': supervised learning loss\n",
    "        \"\"\"\n",
    "        # TODO: update the policy and return the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        action_pred = self.forward(observations)\n",
    "        # print(f'action shape: {action_pred.shape}')\n",
    "        # print(f'truth shape: {actions.shape}')\n",
    "        loss = F.mse_loss(action_pred,actions)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return {\n",
    "            # You can add extra logging information here, but keep this line\n",
    "            'Training Loss': ptu.to_numpy(loss),\n",
    "        }\n",
    "        \n",
    "    def get_action(self, obs: np.ndarray) -> np.ndarray:\n",
    "        obs = torch.tensor(obs,dtype=torch.float32).to(ptu.device)\n",
    "        with torch.no_grad():\n",
    "            action = ptu.to_numpy(self.forward(obs))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many differences such as how to get action and how to compute the loss. Imitaion Learning output the action directly and compute the mse loss that is different with policy gradient which use the gradient of J function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the analysis, I think critic skills we should master are: \n",
    "\n",
    "- computing the expectation of policy gradient\n",
    "  \n",
    "- computing the varience of policy gradient\n",
    "\n",
    "we are able to reduce the varience of this method only if we master these math skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function of policy gradient is formulized:\n",
    "\n",
    "$J(\\theta) = \\mathbb{E}_{\\tau\\sim p_\\theta(\\tau)} \\left[ \\sum_t r(s_t, a_t) \\right] \\approx \\frac{1}{N} \\sum_i \\sum_t r(s_{i,t}, a_{i,t})$\n",
    "\n",
    "where policy gradient aims that good stuff is made more likely and bad stuff is made less likely.\n",
    "\n",
    "Let's compute the gradient(derivative) of this objective function\n",
    "\n",
    "$\\theta^* = \\underset{\\theta}{\\mathrm{arg\\,max}} \\, \\mathbb{E}_{\\tau\\sim p_\\theta(\\tau)} \\left[ \\sum_t r(s_t, a_t) \\right]$\n",
    "\n",
    "$J(\\theta) = \\mathbb{E}_{\\tau\\sim p_\\theta(\\tau)} [r(\\tau)] = \\int p_\\theta(\\tau)r(\\tau) d\\tau$\n",
    "\n",
    "$\\nabla_\\theta J(\\theta) = \\int \\nabla_\\theta p_\\theta(\\tau)r(\\tau) d\\tau = \\int p_\\theta(\\tau) \\nabla_\\theta \\log p_\\theta(\\tau)r(\\tau) d\\tau = \\mathbb{E}_{\\tau\\sim p_\\theta(\\tau)} [\\nabla_\\theta \\log p_\\theta(\\tau)r(\\tau)]$\n",
    "\n",
    "$\\text{a convenient identity} \\qquad p_\\theta(\\tau) \\nabla_\\theta \\log p_\\theta(\\tau) = p_\\theta(\\tau) \\frac{\\nabla_\\theta p_\\theta(\\tau)}{p_\\theta(\\tau)} = \\nabla_\\theta p_\\theta(\\tau)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
